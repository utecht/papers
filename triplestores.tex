\documentclass{llncs}
\usepackage{llncsdoc}
\usepackage{listings}
\lstset{frame=single,language=SQL,morekeywords={PREFIX},basicstyle=\footnotesize\ttfamily}
\usepackage{cite}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern} % load a font with all the characters
\title{RDF Triple Store Performance For Client-side Applications}
\author{Joseph Utecht \and Mathias Brochhausen PhD}
\institute{Department of Biomedical Informatics, University of Arkansas for Medical Sciences, Little Rock, AR}
\date{July 2015}
\begin{document}

\maketitle
\section{Introduction}

Trauma Centers and trauma systems have been proven to increase the positive outcome in trauma care \cite{pmid16427544},\cite{pmid21206286},\cite{pmid26151508}. However, comparison of trauma centers and of trauma systems and the patient outcome created by them is obstructed by a lack of comparability regarding the organizational components and the terms used to refer to organizational structures and roles. The NIH-funded project \textit{Comparative Assessment For Environments of trauma care} (CAFE) (1R01GM111324-01A1)  aims to develop a semantically driven, web-based IT framework to collect data about the organizational structure of trauma centers and trauma system and to allow comparison of different structures for either trauma centers or trauma systems.To achieve that CAFE will create knowledge resource based on an RDF triple store to store data about the organizational structure of both trauma centers and trauma systems. Knowledge about types of entities relevant to the domain will be provided by an ontology coded in Web Ontology Language 2 (OWL2) \cite{OWL2}. The system will allow users to enter data about the organizational structure of a trauma center or a trauma system. The information will be captured in RDF and the progress in entering data will be perceivable to user by the growing graphical representation of the organizational structure. In the next step the user can request a comparison of their organization with canonical organizations of the same type. The data about those organizations will be stored in the triple store. The system will run a set of queries that retrieve data that allows representing the same organizational units described by the users data and their interrelations. The user will be presented with two graphical representations: their own organization and a canonical organization of the same type.

Client driven web applications are becoming increasingly popular \cite{Fielding2000}.  For a server to support a client-side application performance over large numbers of small queries is important to keep the user from waiting on information to populate on their end.  This use for RDF triple stores is not in-line with their standard use and presents some interesting problems \cite{Battle2008}. WE SHOULD SPECIFY WHAT THOSE PROBLEMS ARE AND HOW THEY ARE RELEVANT TO WHAT WE DO.

In this paper...

\section{Material and Methods}
\subsection{RDF Triple Stores}

We have decided to focus on Apache Jena \cite{Jena}, Blazegraph \cite{Blazegraph}, and Sesame \cite{Sesame} RDF stores for our testing due to their support for RDFS reasoning, open source licenses, ability to handle large datasets and REST endpoints for interaction. \cite{Voigt2012}

Testing was performed on VirtualBox VM \cite{Virtualbox} with 2x 2.4 GHz 6-core Xeon processors and 8GB of ram.  The operating system was CentOS 7 \cite{Centos} and all of the stores were hosted on the Java application server Tomcat \cite{Tomcat}.

Apache Jena uses a Java based front-end called Fuseki distributed under the Apache License 2.0.  Fuseki can either be run standalone or under a Java application server.  For the testing the most recent version Fuseki 2.0.0 under Tomcat was used with RDFS level reasoning.  Jena is highly customizable as to graph storage and inference model.

Systrap's Blazegraph (formerly known as Bigdata) is distributed under either the GPLv2 or a commercial license.  It can also either be run as a standalone or in a Java application server.  Blazegraph 1.5.1 under Tomcat was used for testing with a triple + inference graph.  Blazegraph uses up to six indexes to store the data and has a mature query optimizer \cite{RDFDatabaseSystems}.  If inference is enabled Blazegraph will materialize the inferences with a context which allows them to be removed if the RDF classes are modified.

Much like Jena, Sesame is a framework that also includes a web front-end and native triple store.  Sesame version 2.8.4 was used for testing with a native store and RDFS inference enabled.  Sesame allows arbitrary indexes to be created when the graph is initialized we kept the default two.  Like Blazegraph, Sesame materializes RDF inference.

\subsection{Testing Method}
We used Lehigh University Benchmark (LUBM) \cite{Guo2005} generated data to test performance and capability of the triple stores.  We did not use the default testing queries with this dataset as they were more designed to measure the performance of OWL inference models, however this benchmark gave us an arbitrarily large set of data with RDFS classes that we could use for load testing.

To service a high performing client-side application the triple store must be able to both return queries quickly and in large volume.  We built a testing framework to benchmark the triple stores we were testing.  This framework measured two areas, data loading time and throughput of small queries.

Data load time was measured by converting the LUBM dataset into groups of 10 n3 formatted triples and then measuring both the time it took to load one million of those groups and the average load time per group of triples.  These were loaded through the HTTP REST interfaces for each of the stores and the times were measured from the python script that was loading them.

Large queries were used to generate lists and as a starting point for more queries that would pull details out.  To this end we ask for the URIs of all students, professors, classes, departments and universities.  Once we have these we produce five different versions of 1,000 smaller queries to ask for details using increasingly complex queries.

\subsection{Queries}
To begin a query to determine the minimum time that each system will be able to return a query is needed.  Therefor we designed a query that would return a single object with a subject level lookup.  STUDENT is replaced with the URI of a random student pulled from a list of all students.

\begin{lstlisting}[caption=Benchmark Query]
PREFIX ub: <http://swat.cse.lehigh.edu/onto/univ-bench.owl#>
SELECT ?name
WHERE { <STUDENT> ub:name ?name . }
\end{lstlisting}

\smallskip

Query 1 simulates returning the details about an individual where fields are not required.  The OPTIONAL keyword can potentially cause long query runtimes so this is a naive query.  STUDENT is replaced with the URI of a random student pulled from a list of all students.

\begin{lstlisting}[caption=Query 1]
PREFIX ub: <http://swat.cse.lehigh.edu/onto/univ-bench.owl#>
SELECT ?name ?advisor ?email ?telephone
WHERE { optional {?x ub:name ?name .}
        optional {?x ub:advisor ?advisor .}
        optional {?x ub:email ?email .}
        optional {?x ub:telephone ?telephone .}
        values ?x { <STUDENT> }
        }
\end{lstlisting}

\smallskip

Query 2 returns all of the URIs of the authors on a given paper.  This is testing return time where multiple rows will be returned.  PAPER is replaced with a random URI of a paper.

\begin{lstlisting}[caption=Query 2]
PREFIX ub: <http://swat.cse.lehigh.edu/onto/univ-bench.owl#>
SELECT ?author
WHERE
{?author ub:publicationAuthor <PAPER> .
  }
\end{lstlisting}
\smallskip

Query 3 also returns multiple rows except it also has a join involved.  PROFESSOR is replaced with a random URI of a professor.

\begin{lstlisting}[caption=Query 3]
PREFIX ub: <http://swat.cse.lehigh.edu/onto/univ-bench.owl#>
SELECT ?student
WHERE {  ?student ub:takesCourse ?course .
         <PROFESSOR> ub:teacherOf ?course .
      }
\end{lstlisting}

\smallskip

Query 4 is a highly selective query with multiple joins to test inference as both faculty and student are inferred RDF-level relationships.  PROFESSOR is replaced with a random URI of a professor.

\begin{lstlisting}[caption=Query 4]
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX ub: <http://swat.cse.lehigh.edu/onto/univ-bench.owl#>
SELECT ?student ?faculty
WHERE
{?student rdf:type ub:Student .
  ?faculty rdf:type ub:Faculty .
  ?student ub:advisor ?faculty 
  values ?faculty { <PROFESSOR> }
 }
\end{lstlisting}
\clearpage

\section{Results}
\subsection{Load Test}
\begin{table}
\begin{center}
\caption{Data load times for 10 rows}
\begin{tabular}{l | r r r r }
Store & Total & Mean & Stdev & Median \\
\hline
Jena & 1776k & 13 & 101 & 5 \\
Blazegraph & 39341k & 309 & 429 & 12 \\
Sesame & 8414k & 66 & 205 & 35
\end{tabular}
\\[5pt]
Measured in milliseconds
\end{center}
\end{table}
The results of the loading test show a large difference between the three storage models of the triple stores.  Blazegraph which has the largest number of indexes takes the longest to insert new data into.  Sesame which materializes the inferences being made also takes longer than Jena which is doing inferences at query runtime.

\subsection{Queries}
\begin{table}
\begin{center}
\caption{Mean runtime for 1,000 queries}
\begin{tabular}{l | r r r r r}
    Store & Baseline Query $\overline{x}$ & Query 1 $\overline{x}$ & Query 2 $\overline{x}$ & Query 3 $\overline{x}$ & Query 4 $\overline{x}$ \\
\hline
Jena & 8 & 4413 & 7 & 1678 & 151 \\
Blazegraph & 38 & 36 & 37 & 36 & 38 \\
Sesame & 8 & 253 & 8 & 8 & 7
\end{tabular}
\\[5pt]
Measured in milliseconds
\end{center}
\end{table}

Jena's performance was highly volatile based on the query.  The baseline speed was very fast as expected from the low number of indexes and the lack of inference in the baseline query.  The OPTIONAL keyword in the first query is known to cause potential slowdowns and takes a heavy toll on Jena.  Query 3 also has very poor performance but the reason for this is not clear.  The inference required for query 4 slows it down in comparison to both Blazegraph and Sesame as it must calculate the inference at query time.

Blazegraph's large number of indexes and well established query optimizer result in extremely stable query results.  There appears to be a 30ms processing time on anything Blazegraph is doing, this could be the query optimizer or another part of the system. 

Sesame showed the best performance overall only having problems with the OPTIONAL keyword in the first query.

\section{Discussion}
In client-side web applications response time is important for keeping users happy.  Understanding why triple stores perform the way they do is helpful in choosing the correct one for any given project.  We have demonstrated the large performance differences that can appear even in relatively small datasets between the different index and inference models.

\bibliography{mybib}
\bibliographystyle{ieeetr}
\end{document}
