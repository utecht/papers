\documentclass{llncs}
\usepackage{llncsdoc}
\usepackage{listings}
\lstset{frame=single,language=SQL,morekeywords={PREFIX},basicstyle=\footnotesize\ttfamily}
\usepackage{cite}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern} % load a font with all the characters
\title{Measuring the Usability of Triple Stores for Knowledge Management on Trauma Care Organizations}
\author{Joseph Utecht \and Mathias Brochhausen PhD}
\institute{Department of Biomedical Informatics, University of Arkansas for Medical Sciences, Little Rock, AR}
\date{July 2015}
\begin{document}

\maketitle

\begin{abstract}
The CAFE project aims to provide a semantic web technology-based approach to compare the organizational structures of trauma centers and trauma systems. To achieve this we plan to use an RDF triple store that employs automatic inferences based on OWL representations. In order to engage users with the CAFE application real time feedback is a requirement. Although many RDF triple store performance measure have been published, there appears to be a gap when it comes to their use as the primary storage for real time applications.  The performance needs for this use case differ from the triple stores more traditional use of offline reasoning and inference over large data sets. The objective of this research is to determine the feasibility of modern RDF triple stores as the primary storage for a real time application. To make this determination we measured the load time of just over one million triples of synthetic data and then ran various queries to emulate the types of demands this use would create. The resulting time measurements showed a difference in load times between the different stores and a large range of results in query performance.  The lower end results are fast enough to compete with traditional relational databases while the results on the high end would not be suitable for this use case. The conclusion of this for the CAFE project is that we will be able to use a triple store, but we need to use the performance implications gained from this study to carefully tailor the queries we will run.
\end{abstract}
\section{Introduction}

Trauma Centers and trauma systems have been proven to increase positive outcomes in trauma care \cite{pmid16427544, pmid21206286, pmid26151508}. However, comparison of trauma centers and the patient outcome created by them is obstructed by a lack of comparability regarding the organizational components and the terms used to refer to organizational structures and roles. The NIH-funded project \textit{Comparative Assessment For Environments of trauma care} (CAFE) (1R01GM111324-01A1) aims to develop a semantically driven, web-based IT framework to collect data about the organizational structure of trauma centers and systems to allow comparison of different organizational structures. To achieve that CAFE will create a knowledge resource based on an RDF triple store to store data about the organizational structure of both trauma centers and trauma systems. Knowledge about types of entities relevant to the domain will be provided by an ontology coded in Web Ontology Language 2 (OWL2) \footnote{\url{http://www.w3.org/TR/owl2-primer/}}. The Ontology of Trauma Care Organizational Structures is currently under development. This process is guided by domain experts guiding the domain analysis and reviewing the OWL file. Our system will allow users to enter data about the organizational structure of a trauma center or system. This information will be captured from a web application in RDF and the progress in entering data will be perceivable to user by the growing graphical representation of the organizational structure. In the next step the user can request a comparison of their organization with canonical organizations of the same type. The data about those organizations will be stored in the triple store. The system will run a set of queries that retrieve data allowing representations of the same organizational units described by the users data and their interrelations. After the process of evaluation the data entered will be added to the triple store for future usage.

The CAFE project requires the ability to insert, reason and query data in as close to real time as is possible so the user experience does not suffer. Research has shown that even small delays in a website while attempting to perform some task will greatly decrease the rate at which people complete said task \cite{Galletta2002}.  We see two methods of accomplishing this: A) a relational database to serve the application that is populated through offline reasoning, or B) real time reasoning provided by an RDF triple store that is populated directly from the application. Method A is the more commonly applied however to simplify the architecture of CAFE we would like to use method B, where the triple store is utilized as the primary data storage. While other research has measured the performance of triple stores for reasoning or complex queries, that only covers half of our use case.  We need to know the query throughput of simple queries that modern triple stores can produce so that when designing the CAFE application we will know the upper limit of queries per page. In this paper we measure the performance of various RDF triple stores as they would be used to drive a real time application. We also compare the performance to that of a relational database.

\section{Material and Methods}
\subsection{RDF Triple Stores}

We have decided to focus on Apache Jena \footnote{\url{https://jena.apache.org/}}, Blazegraph \footnote{\url{http://www.blazegraph.com/bigdata}}, and Sesame \footnote{\url{http://rdf4j.org/}}  RDF stores for our testing due to their support for RDFS reasoning, open source licenses, ability to handle large datasets, and REST endpoints for interaction. \cite{Voigt2012}

Testing was performed on a VirtualBox VM \footnote{\url{https://www.virtualbox.org/}} with 2x 2.4 GHz 6-core Xeon processors and 8GB of ram.  The operating system was CentOS 7 \footnote{\url{https://www.centos.org/}} and all of the stores were hosted on the Java application server Tomcat \footnote{\url{http://tomcat.apache.org/}}.

Apache Jena uses a Java based front-end called Fuseki distributed under the Apache License 2.0.  Fuseki can either be run standalone or under a Java application server.  For testing, the most recent version Fuseki 2.0.0 with Tomcat was used with RDFS level reasoning.

Systrap's Blazegraph (formerly known as Bigdata) is distributed under either the GPLv2 or a commercial license.  It can also either be run as a standalone or in a Java application server.  Blazegraph 1.5.1 under Tomcat was used for testing with a triple + inference graph.  Blazegraph uses up to six indexes to store the data and has a mature query optimizer \cite{RDFDatabaseSystems}.  If inference is enabled Blazegraph will materialize the inferences with a context which allows them to be removed if the RDF classes are modified.

Much like Jena, Sesame is a framework that also includes a web front-end and native triple store.  Sesame version 2.8.4 was used for testing with a native store and RDFS inference enabled.  Sesame allows arbitrary indexes to be created when the graph is initialized but we kept the default two.  Like Blazegraph, Sesame materializes RDF inference when data is added.

\subsection{Testing Method}
We used Lehigh University Benchmark (LUBM) \cite{Guo2005} generated data to test performance and capability of the triple stores. LUBM was used to produce a synthetic dataset of arbitrary size, that represents people and activities in an academic setting, such as students, professors, classs, departments, and universities.  We did not use the default testing queries with this dataset as they were more designed to measure the performance of OWL inference models.

To service a high performing client-side application the triple store must be able to both return queries quickly and in large volume.  We built a testing framework to benchmark the triple stores, measuring two areas, data loading time and throughput of small queries.

Data load time was measured by converting the LUBM dataset into groups of 10 n3 formatted triples and then measuring the time it took to load one million of those groups.  These were loaded through the HTTP REST interfaces for each of the stores and the times were measured from the python script that was loading them.

SPARQL queries were used to generate lists of URIs as a starting point for the small queries with which we were testing query runtime.  To this end we queried for the URIs of all students, professors, classes, departments and universities.  Once we had these we ran the five queries below 1,000 times through the HTTP REST interface measuring the response time with a python script.

\subsection{Relational Database Baseline}
To have a baseline to measure the performance of the RDF triple stores we decided to use a relational database performing close to the same task.  To accomplish this we converted a subset of the LUBM data into a relational format and loaded it into a MariaDB \footnote{\url{https://mariadb.org/}} installation on an identical VM to the triple stores. Once the data was converted and loaded, indexes were places upon the columns which would act as keys. To make the comparison closer, a miniature HTTP REST interface was created in Python's Flask framework \footnote{\url{http://flask.pocoo.org/}}. This interface would take a key as an HTTP parameter and then run a SQL query to the database returning the results as JSON encoded data \footnote{\url{http://json.org/}}. As we were not interested in attempting a comparison between the relative run times of SQL vs SPARQL queries only two simple SQL queries were used to determine the baseline time this relational database setup would return data.

\subsection{Queries}
To begin, a query to determine the baseline time that each system will be able to return a query is needed.  Therefor we designed a query that would return a single object with a subject level lookup, which should be the fastest possible lookup operation for any store.  STUDENT is replaced with the URI of a random student pulled from a list of all students.

\begin{lstlisting}[caption=Benchmark Query]
PREFIX ub: <http://swat.cse.lehigh.edu/onto/univ-bench.owl#>
SELECT ?name
WHERE {
        <STUDENT> ub:name ?name .
      }
\end{lstlisting}

\smallskip

Query 1 simulates returning the details about an individual where fields are not required to be present.  The \emph{optional} keyword can potentially cause long runtimes so this is a naive approach to this query.  In the query STUDENT is replaced with the URI of a random student pulled from a list of all students.

\clearpage

\begin{lstlisting}[caption=Query 1]
PREFIX ub: <http://swat.cse.lehigh.edu/onto/univ-bench.owl#>
SELECT ?name ?advisor ?email ?telephone
WHERE {
        optional {?x ub:name ?name .}
        optional {?x ub:advisor ?advisor .}
        optional {?x ub:email ?email .}
        optional {?x ub:telephone ?telephone .}
        values ?x { <STUDENT> }
      }
\end{lstlisting}

\smallskip

Because of the performance implications of the \emph{optional} keyword a second method for querying of the same information was performed.  Query 1.5 was actually 4 separate queries each asking for one piece of the original data returned by query 1.  Time on this query was a measure of how long it took to perform all 4 queries.

\begin{lstlisting}[caption=Sample of Query 1.5]
PREFIX ub: <http://swat.cse.lehigh.edu/onto/univ-bench.owl#>
SELECT ?name
WHERE {
        <STUDENT> ub:name ?name .
      }
\end{lstlisting}

\smallskip

Query 2 returns all of the URIs of the authors on a given paper.  This is testing return time where multiple rows will be returned.  PAPER is replaced with a random URI of a paper.

\begin{lstlisting}[caption=Query 2]
PREFIX ub: <http://swat.cse.lehigh.edu/onto/univ-bench.owl#>
SELECT ?author
WHERE {
        ?author ub:publicationAuthor <PAPER> .
      }
\end{lstlisting}
\smallskip

Query 3 also returns multiple rows and also has a join involved.  PROFESSOR is replaced with a random URI of a professor.

\begin{lstlisting}[caption=Query 3]
PREFIX ub: <http://swat.cse.lehigh.edu/onto/univ-bench.owl#>
SELECT ?student
WHERE {
        ?student ub:takesCourse ?course .
        <PROFESSOR> ub:teacherOf ?course .
      }
\end{lstlisting}

\smallskip

Query 4 is a highly selective query to test inference as both ub:Faculty and ub:Student are inferred RDF-level relationships.

\begin{lstlisting}[caption=Query 4]
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX ub: <http://swat.cse.lehigh.edu/onto/univ-bench.owl#>
SELECT ?student ?faculty
WHERE {
        ?student rdf:type ub:Student .
        ?faculty rdf:type ub:Faculty .
        ?student ub:advisor ?faculty 
        values ?faculty { <PROFESSOR> }
       }
\end{lstlisting}

\section{Results}
The relational database baseline returned on average in 5ms for a single row lookup and 6ms for a join.

\begin{table}
\begin{center}
\caption{Data load times for 10 triples}
\begin{tabular}{l | r r r r }
Store & Total & Mean & Stdev & Median \\
\hline
Jena & 1776k & 13 & 101 & 5 \\
Blazegraph & 39341k & 309 & 429 & 12 \\
Sesame & 8414k & 66 & 205 & 35
\end{tabular}
\\[5pt]
Measured in milliseconds
\end{center}
\end{table}
The results of the loading test show a large difference between the three storage models of the triple stores.  Blazegraph which has the largest number of indexes takes the longest to insert new data into. Sesame and Jena have similar numbers of indexes, however Sesame is materializing the inferences and thus has slower load times.

\begin{table}
\begin{center}
\caption{Mean runtime for 1,000 queries}
\begin{tabular}{l | r | r | r | r | r | r}
    Store & Baseline $\overline{x}$ & Query 1 $\overline{x}$ & Query 1.5 $\overline{x}$ & Query 2 $\overline{x}$ & Query 3 $\overline{x}$ & Query 4 $\overline{x}$ \\
\hline
Jena & 8 & 4413 & 29 & 7 & 1678 & 151 \\
Blazegraph & 38 & 36 & 146 & 37 & 36 & 38 \\
Sesame & 8 & 253 & 29 & 8 & 8 & 7
\end{tabular}
\\[5pt]
Measured in milliseconds
\end{center}
\end{table}

Jena's performance was highly volatile based on the query.  The baseline speed was very fast as expected from the low number of indexes and the lack of inference in the baseline query.  The \emph{optional} keyword in the first query is known to cause potential slowdowns and takes a heavy toll on Jena.  Query 3 also has very poor performance but the reason for this is not clear.  The inference required for query 4 slows it down in comparison to both Blazegraph and Sesame as it must calculate the inference at query time.

Blazegraph's large number of indexes and well established query optimizer result in extremely stable query time.  Blazegraph was the only store that did not take a performance hit from the \emph{optional} keyword in the first query.  There however appears to be a 30ms processing time on anything Blazegraph is doing, of which we were unable to locate the cause. 

Sesame showed the best performance overall only having problems with the \emph{optional} keyword in the first query.

\section{Conclusion}
This paper examined the performance of RDf triple stores for query throughput in a real time application and compared it to the performance of a relational database.  We found that performance in two of the stores, Jena and Sesame, were within a few milliseconds as a highly optimized relational databases for some queries.  Based on the performance we will move forward with our plans to use an RDF triple store as the primary storage for the real time web application in the CAFE project.

\medskip
\noindent
\textbf{Acknowledgement} Research reported in this publication was supported by the National Institute of General Medical Sciences of the National Institutes of Health under award number 1R01GM111324.

\bibliography{mybib}
\bibliographystyle{ieeetr}
\end{document}
